{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3cd676-a8a3-4df6-9502-6a660449d0e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image Retrieval From Text\n",
    "\n",
    "Now that we can process a video and find it's encoded vector, I want to be able to match images to a given text input. Instead of matching an image to a label, we want to reverse the process. I want to do this by maximizing the dot product between the encoded vectors for the images and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef406d86-3d19-4873-b3a7-6ab60e93e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8909dea5-4f21-4bda-83e0-edb062413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# To test this on, we will use the CIFAR dataset again.\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34676c6e-8a3b-4372-a574-dc0222027acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we will use the first 10 images in the dataset\n",
    "images = [image for image, class_id in cifar100][:10]\n",
    "images.append(Image.open(\"frame0.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d57faa15-2907-47d7-b73f-eb6d69cb8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we can use the function we already defined to do this, except we are\n",
    "# inputting images, so we modify it a bit\n",
    "def apply_clip(frames, model, preprocess, device = \"cpu\"):\n",
    "    image_inputs = [preprocess(image).unsqueeze(0).to(device) for image in frames]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = [model.encode_image(image_input) for image_input in image_inputs]\n",
    "        \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "020f7e56-5404-461e-9b2d-b1947e667bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1411fd8-9c15-402d-9607-fe991b8c7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.cat(apply_clip(images, model, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35cf1e-dc9b-4310-95a4-5fede43eaad1",
   "metadata": {},
   "source": [
    "Now we have a list of encoded vectors, exactly what we want. Let's create a text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3fee87f4-11e9-4a3f-80d9-1fb16e6658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = clip.tokenize(\"a photo of an apple\")\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab7c21-ef51-4bb1-88f4-96a9352225ec",
   "metadata": {},
   "source": [
    "Now let's find the dot product between this and each encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ad9bf78-f895-4799-a4c7-db75558dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First normalize each vector\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True) \n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9cd92448-2a07-4992-9d13-1fd522b324d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features @ text_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a78a68f-5793-4cd6-ba9c-5305e772460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (100 * text_features @ image_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "86fc9dd4-26b2-4f36-b872-ed465395b96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.6555e-04, 1.1019e-02, 2.5970e-03, 5.4669e-02, 5.5438e-02, 5.2825e-02,\n",
       "         6.5651e-03, 7.8042e-03, 2.2690e-02, 7.8572e-01, 3.9903e-06]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c85ef140-479f-4cc8-8302-71a5425b0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = similarity[0].topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9cf1103-06e2-40bc-84c2-8e5a77926d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857238054275513"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "44b6f3b2-6bce-4507-86a8-b11bc21b99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 78.57%, the top image is:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE53B58ECA0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"With {100*values[0].item():.2f}%, the top image is:\")\n",
    "images[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bff0e-5bee-4ea9-9e38-4fb0cd3dc659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
