{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3cd676-a8a3-4df6-9502-6a660449d0e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image Retrieval From Text\n",
    "\n",
    "Now that we can process a video and find it's encoded vector, I want to be able to match images to a given text input. Instead of matching an image to a label, we want to reverse the process. I want to do this by maximizing the dot product between the encoded vectors for the images and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef406d86-3d19-4873-b3a7-6ab60e93e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8909dea5-4f21-4bda-83e0-edb062413388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# To test this on, we will use the CIFAR dataset again.\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34676c6e-8a3b-4372-a574-dc0222027acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we will use the first 10 images in the dataset\n",
    "images = [image for image, class_id in cifar100][:10]\n",
    "images.append(Image.open(\"frame0.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d57faa15-2907-47d7-b73f-eb6d69cb8279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we can use the function we already defined to do this, except we are\n",
    "# inputting images, so we modify it a bit\n",
    "def apply_clip(frames, model, preprocess, device = \"cpu\"):\n",
    "    image_inputs = [preprocess(image).unsqueeze(0).to(device) for image in frames]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = [model.encode_image(image_input) for image_input in image_inputs]\n",
    "        \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "020f7e56-5404-461e-9b2d-b1947e667bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1411fd8-9c15-402d-9607-fe991b8c7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.cat(apply_clip(images, model, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35cf1e-dc9b-4310-95a4-5fede43eaad1",
   "metadata": {},
   "source": [
    "Now we have a list of encoded vectors, exactly what we want. Let's create a text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3fee87f4-11e9-4a3f-80d9-1fb16e6658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = clip.tokenize(\"a photo of a camel\")\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab7c21-ef51-4bb1-88f4-96a9352225ec",
   "metadata": {},
   "source": [
    "Now let's find the dot product between this and each encoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8ad9bf78-f895-4799-a4c7-db75558dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First normalize each vector\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True) \n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2a78a68f-5793-4cd6-ba9c-5305e772460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = (100 * text_features @ image_features.T).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86fc9dd4-26b2-4f36-b872-ed465395b96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2930e-04, 1.0709e-03, 2.0246e-02, 3.7623e-03, 2.8802e-03, 2.2603e-04,\n",
       "         9.6594e-01, 3.6589e-03, 1.6958e-03, 2.7683e-04, 9.4071e-06]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c85ef140-479f-4cc8-8302-71a5425b0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = similarity[0].topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f9cf1103-06e2-40bc-84c2-8e5a77926d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659440517425537"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "44b6f3b2-6bce-4507-86a8-b11bc21b99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 96.59%, the top image is:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE53B142430>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"With {100*values[0].item():.2f}%, the top image is:\")\n",
    "images[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25af7cc-deac-48c7-bfa1-e0dd59a6dbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
